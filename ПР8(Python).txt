from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
X, y = mglearn.datasets.make_wave(n_samples = 100)
line = np.linspace(-3,3,1000, endpoint = False).reshape(-1, 1)
reg = DecisionTreeRegressor(min_samples_split=3).fit(X, y)
plt.plot(line, reg.predict(line), label = "дерево решений")
reg = LinearRegression().fit(X, y)
plt.plot(line, reg.predict(line), label = "линейная регрессия")
plt.plot(X[:,0], y, 'o', c = 'k')
plt.ylabel("Выход регрессии")
plt.xlabel("Входной признак")
plt.legend(loc = "best")
bins = np.linspace(-3,3,11)
print("категории: {}".format(bins))
which_bin = np.digitize(X, bins=bins)
print("\nТочки данных:\n", X[:5])
print("\nКатегории для точек данных:\n", which_bin[:5])




from sklearn.preprocessing import OneHotEncoder
# преобразовываем с помощью OneHotEncoder
encoder = OneHotEncoder(sparse=False)
# encoder.fit находит уникальные значения, имеющиеся в which_bin
encoder.fit(which_bin)
# transform осуществляет прямое кодирование
X_binned = encoder.transform(which_bin)
print(X_binned[:5])
Out[14]:
[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]
print("формамассива X_binned: {}".format(X_binned.shape))
line_binned = encoder.transform(np.digitize(line, bins = bins))
reg = LinearRegression().fit(X_binned, y)
plt.plot(line, reg.predict(line_binned), label = 'линейная регрессия после биннинга')
reg = DecisionTreeRegressor(min_samples_split = 3).fit(X_binned, y)
plt.plot(line, reg.predict(line_binned), label = 'дерево решений после биннинга')
plt.plot(X[:,0], y, 'o', c = 'k')
plt.vlines(bins, -3, 3, linewidth = 1, alpha = .2)
plt.legend(loc = "best")
plt.ylabel("Выход регрессии")
plt.xlabel("Входной признак")





X_combined = np.hstack([X, X_binned])
print(X_combined.shape)
Out[17]:
(100,11)
In[18]:
reg = LinearRegression().fit(X_combined, y)
line_combined = np.hstack([line, line_binned])
plt.plot(line, reg.predict(line_combined),
label = 'линейная регрессия после комбинирования')
forbinin bins:
plt.plot([bin,bin],[-3,3], ':', c = 'k')
plt.legend(loc="best")
plt.ylabel("Выход регрессии")
plt.xlabel("Входной признак")
plt.plot(X[:,0], y, 'o', c = 'k')
X_product=np.hstack([X_binned,X*X_binned])
print(X_product.shape)
Out[19]:
(100,20)

reg = LinearRegression().fit(X_product, y)
line_product = np.hstack([line_binned, line * line_binned])
plt.plot(line, reg.predict(line_product), label='линейная регрессия произведение')
for bin in bins:
plt.plot([bin,bin],[-3,3],':', c='k')
plt.plot(X[:,0], y,'o', c='k')
plt.ylabel("Выход регрессии")
plt.xlabel("Входной признак")
plt.legend(loc="best")


from sklearn.preprocessing import PolynomialFeatures
# задаем степень полинома 10:
# значение по умолчанию "include_bias=True" добавляет признак-константу 1
poly = PolynomialFeatures(degree=10, include_bias=False)
poly.fit(X)
X_poly = poly.transform(X)
print("форма массива X_poly: {}".format(X_poly.shape))
Out[22]:
форма массива X_poly:(100,10)
print("Элементы массива X:\n{}".format(X[:5]))
print("Элементы массива X_poly:\n{}".format(X_poly[:5]))
Out[23]:
Элементы массива X:
[[-0.753]
[2.704]
[1.392]
[0.592]
[-2.064]]
Элементы массива X_poly:
[[-0.753 0.567 -0.427 0.321 -0.242 0.182 -0.137 0.103 -0.078 0.058]
[2.704 7.313 19.777 53.482 144.632 391.125 1057.714 2860.360 7735.232 20918.278]
[1.392 1.938 2.697 3.754 5.226 7.274 10.125 14.094 19.618 27.307]
[0.592 0.350 0.207 0.123 0.073 0.043 0.025 0.015 0.009 0.005]
[-2.064 4.260 -8.791 18.144 -37.448 77.289 -159.516 329.222 -679.478 1402.367]]
print("Имена полиномиальных признаков:\n{}".format(poly.get_feature_names()))
Out[24]:
Имена полиномиальных признаков:
['x0','x0^2','x0^3','x0^4','x0^5','x0^6','x0^7','x0^8','x0^9','x0^10']
reg = LinearRegression().fit(X_poly, y)
line_poly = poly.transform(line)
plt.plot(line, reg.predict(line_poly),
label='полиномиальная линейная регрессия')
plt.plot(X[:,0], y,'o', c='k')
plt.ylabel("Выход регрессии")
plt.xlabel("Входной признак")
plt.legend(loc="best")



import pandas as pd
# Файл не содержит заголовков столбцов, поэтому мы передаем header=None
# и записываем имена столбцов прямо в "names"
data = pd.read_csv(
"C:/Data/adult.data", header=None, index_col=False,
names=['age', 'workclass', 'fnlwgt', 'education', 'education-num',
'marital-status', 'occupation', 'relationship', 'race', 'gender',
'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',
'income'])
# В целях упрощения мы выберем лишь некоторые столбцы
data = data[['age', 'workclass', 'education', 'gender', 'hours-per-week‘, 'occupation', 'income']]
# IPython.display позволяет вывести красивый вывод, отформатированный в Jupyter notebook
display(data.head())
print(data.gender.value_counts())
Out[3]:
Male 21790
Female 10771
Name: gender, dtype: int64
print("Исходные признаки:\n",list(data.columns),"\n")
data_dummies = pd.get_dummies(data)
print("Признаки после get_dummies:\n",list(data_dummies.columns))
Out[4]:
Исходныепризнаки:
['age', 'workclass', 'education', 'gender', 'hours-per-week', 'occupation', 'income']
Исходные признаки после get_dummies:
['age', 'hours-per-week', 'workclass_ ?', 'workclass_ Federal-gov', 'workclass_ Local-gov', 'workclass_
Never-worked','workclass_ Private', 'workclass_ Self-emp-inc','workclass_ Self-emp-not-inc', 'workclass_
State-gov','workclass_ Without-pay','education_ 10th', 'education_ 11th','education_ 12th','education_
1st-4th',
...
'education_ Preschool','education_ Prof-school','education_ Some-college', 'gender_ Female','gender_
Male','occupation_ ?', 'occupation_ Adm-clerical','occupation_ Armed-Forces', 'occupation_ Craftrepair','occupation_ Exec-managerial', 'occupation_ Farming-fishing','occupation_ Handlers-cleaners',
...
'occupation_ Tech-support','occupation_ Transport-moving', 'income_ <=50K','income_ >50K']
data_dummies.head()
# Берем только те столбцы, которые содержат признаки,
# то есть все столбцы, начиная с 'age' и заканчивая 'occupation_ Transport-moving'
# Этот диапазон содержит все признаки, кроме целевой переменной
features = data_dummies.ix[:,'age': 'occupation_ Transport-moving']
# Извлекаем массивы NumPy
X = features.values
y = data_dummies['income_ >50K'].values
print("форма массива X: {} форма массива y: {}".format(X.shape, y.shape))
Out[6]:
форма массива X:(32561,44) форма массива y:(32561,)
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
print("Правильность на тестовомнаборе: {:.2f}".format(logreg.score(X_test, y_test)))